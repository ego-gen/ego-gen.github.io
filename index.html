<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta name="description" content="EgoGen is a generative and scalable synthetic data generation approach specifically tailored for egocentric perception tasks.">
       <meta name="keywords" content="egocentric simulation; human motion synthesis; egocentric view; first-person view; human-scene interaction; deep learning; 3D vision; egocentric computer vision;">
      <meta name="author" content="Gen Li">
      <title>EgoGen: An Egocentric Synthetic Data Generator</title>

      <meta property="og:title" content="EgoGen" />
      <meta property="og:description" content="EgoGen is a generative and scalable synthetic data generation approach specifically tailored for egocentric perception tasks.">
      <meta property="og:image" content="" />

      <!--<meta name="twitter:card" content="summary_large_image" />-->
      <meta name="twitter:title" content="EgoGen" />
      <meta name="twitter:description" content="EgoGen is a generative and scalable synthetic data generation approach specifically tailored for egocentric perception tasks." />
      <meta name="twitter:image" content="" />
      <!--<meta name="twitter:image:alt" content="EgoBody" />-->

      <!-- Bootstrap core CSS -->
      <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
      <!-- Custom styles for this template -->
      <link href="css/scrolling-nav.css" rel="stylesheet">
       <link href="css/hr.css" rel="stylesheet" >
      <!-- nice figures  -->
<!--      <link rel="stylesheet" href="css/font-awesome.css">-->
      <link rel="icon" type="image/png" href="images/logo.png">
<!--       <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>"> -->
    <style>
   #mainNav .navbar-nav .nav-item {
      margin-right: 20px; /* Adjust the value as needed */
   }
</style>
        
        <script async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


        <style>
    /* Custom CSS for showing submenus on hover */
    .nav-item.dropdown:hover .dropdown-menu {
      display: block;
    }
  </style>


   </head>
   <body id="page-top">
      <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
         <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">EgoGen <img src="images/logo.png" style="vertical-align: -3px;" width="20"> </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
               <ul class="navbar-nav ml-auto">
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#about">About</a>
                  </li>

      <!-- Dropdown button with submenus -->
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" href="#method" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
          Human motion model
        </a>
        <div class="dropdown-menu" aria-labelledby="navbarDropdown">
          <!-- Dropdown items with links to different sections -->
          <a class="dropdown-item" href="#training">Training</a>
          <a class="dropdown-item" href="#eval">Evaluation</a>
        </div>
      </li>

                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#rendering">Synthetic data generation</a>
                  </li>

                    <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" href="#applications" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
          Applications
        </a>
        <div class="dropdown-menu" aria-labelledby="navbarDropdown">
          <!-- Dropdown items with links to different sections -->
          <a class="dropdown-item" href="#lamar">Mapping & Localization for AR</a>
          <a class="dropdown-item" href="#hmr">HMR from Egocentric views</a>
        </div>
      </li>

                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#video">Video</a>
                  </li>
                   <!--<li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#downloads">Downloads</a>
                  </li>-->
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#citation">Citation</a>
                  </li>
                  <!--<li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#team">Team</a>
                  </li>-->
               </ul>
            </div>
         </div>
      </nav>


      <header class="bg-light text-black">
          <div class="container text-center">
	      <h7><img src="images/logo.png" width="45"></h7>
              <h1><font color="#015FBD "><b>EgoGen</b></font></h1>
              <h2><font color="#015FBD "><b>An Egocentric Synthetic Data Generator </b></font></h2><br><br>
              <div id="content">
          <div id="content-inner">

            <div class="section head">

                <div class="authors">
                    <h5>
                        <a href="https://ligengen.github.io">Gen Li</a><sup>1</sup>&nbsp;
                        <a href="https://vlg.inf.ethz.ch/team/Kaifeng-Zhao.html">Kaifeng Zhao</a><sup>1</sup>&nbsp;
                        <a href="https://vlg.inf.ethz.ch/team/Siwei-Zhang.html">Siwei Zhang</a><sup>1</sup>&nbsp;
                        <a href="https://vlg.inf.ethz.ch/team/Xiaozhong-Lyu.html">Xiaozhong Lyu</a><sup>1</sup>&nbsp;
                        <a href="https://dusmanu.com/">Mihai Dusmanu</a><sup>2</sup>&nbsp;
                        <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a><sup>1</sup>&nbsp;
                        <a href="https://cvg.ethz.ch/team/Prof-Dr-Marc-Pollefeys">Marc Pollefeys</a><sup>1, 2</sup>&nbsp;
                        <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a><sup>1</sup>
                        <h5>
                </div>

                <div class="affiliations">
                    <h5>
                        <sup>1</sup><a href="https://ethz.ch/en.html">ETH ZÃ¼rich</a>&nbsp; &nbsp;&nbsp;
                        <sup>2</sup><a href="https://www.microsoft.com/en-us/research/">Microsoft</a> &nbsp;&nbsp;
                        
                        <h5>
                </div>

                <br>
                <div class="venue"><h5>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr.thecvf.com/Conferences/2024" target="_blank">CVPR</a>) 2024 <h5></div>
		<div class="venue"><h5> <font color="#EE0303 "><b>Oral Presentation </b></font> <h5></div>


                <div class="downloads">
                    <br><h3>
                    <a class="publink" href="https://arxiv.org/abs/2401.08739" target="_blank" style="text-decoration: none"> Arxiv <i class="fa fa-print"></i></a> &nbsp;
                    &nbsp;&nbsp;
                    <a class="publink" href="https://github.com/ligengen/EgoGen" target="_blank" style="text-decoration: none"> Code <i class="fa fa-github"></i></a> &nbsp;
                    &nbsp;&nbsp;
		    <a class="publink" href="https://egogen.ethz.ch" target="_blank" style="text-decoration: none"> Dataset <i class="fa fa-database"></i></a>
                    <h3>
                </div>
            </div>


        </div>
      </header>


      <section id="about" class="about-section">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                     <!-- <p><img class="img-fluid" alt="teaser" src="https://via.placeholder.com/1500x600"></p> -->
                     <img class="img-fluid" alt="teaser" src="images/teaser-final.png"></p>
                     <h5 class="img-wide text-center">
                        <font color="#6F6D6D " size="4">EgoGen: a scalable synthetic data generation system for egocentric perception tasks, with rich multi-modal data and accurate annotations. We simulate camera rigs for head-mounted devices (HMDs) and render from the perspective of the camera wearer with various sensors. Top to bottom: middle and right camera sensors in the rig. Left to right: photo-realistic RGB image, RGB with simulated motion blur, depth map, surface normal, segmentation mask, and world position for fisheye cameras widely used in HMDs. </font>
                        <h5>
                  <br>
		     <!--<p><video style="width: 100%; height: auto; display: block; margin: auto;" controls autoplay preload loop muted playsinline> <source src="video/teaser.mp4" type="video/mp4"/> </video>-->
                  <div class="embed-responsive embed-responsive-16by9">
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/QbiOJKZ6PBU?si=kdYkxYNZX-SGOa8G&mute=1&autoplay=1&loop=1&playlist=QbiOJKZ6PBU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                  </div>

                  <br>
                   <h2>Abstract</h2>
                   <div class="text-center">
                     <!-- <p><img class="img-fluid" alt="teaser" src="https://via.placeholder.com/1500x600"></p> -->
<!--                     <p><img class="img-fluid" alt="teaser" src="images/teaser.png"></p>-->
                       <p class="lead text-justify">
<!--                           <b>Dataset Overview</b>. -->
                           Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce <i>EgoGen</i>, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of <i>EgoGen</i> is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model offers a closed-loop solution where the embodied perception and movement of the virtual human are seamlessly coupled. Compared to previous works, our model eliminates the need for a pre-defined global path, and is directly applicable to dynamic environments. Combined with our easy-to-use and scalable data generation pipeline, we demonstrate  <i>EgoGen</i>âs efficacy in three tasks: mapping and localization for head-mounted cameras, egocentric camera tracking, and human mesh recovery from egocentric views. <i>EgoGen</i> will be fully open-sourced, offering a practical solution for creating realistic egocentric training data and aiming to serve as a useful tool for egocentric computer vision research. 
                  </p>
                  </div>



               </div>
            </div>
         </div>
      </section>



      <section id="method" class="">
         <div class="container" id="training">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                   <h2>Egocentric perception-driven motion synthesis</h2> <br>
                  <h3>Training in static scenes</h3>
                  <div class="text-center">
                     <!-- <p><img class="img-fluid" alt="teaser" src="https://via.placeholder.com/1500x600"></p> -->
<!--                     <p><img class="img-fluid" alt="teaser" src="images/teaser.png"></p>-->
                       <p class="lead text-justify">
                           As William Gibson stated: <q>We see in order to move; we move in order to see.</q> We close the loop for the interdependence between egocentric synthetic image data and human motion synthesis.
                       </p>

                       <p class="lead text-justify">
                          Our autonomous virtual humans are able to <em>see</em> their environment with <em>egocentric</em> visual inputs, explore by themselves, without being constrained by predefined paths. 
                       </p>
                  </div>
                     <p><img class="img-fluid" alt="method overview" src="images/overview.jpg"></p>
                    <h5 class="img-wide text-center" id="eval">
                        <font color="#6F6D6D " size="4">The policy network learns a generalizable mapping from motion seed body markers \( \mathbf{X}_t^S \), marker directions \( \mathbf{X}_t^{S^D} \), egocentric sensing \( \mathcal{E}_t \), and distance to the target \( d_t \) to CAMPs. The policy learns a stochastic collision avoiding action space to predict future body markers \( \mathbf{X}_t^F \). For illustration purposes, we visualize only one frame of \( \mathbf{X}_t^S \) and \( \mathcal{E}_t \).</font>
                  <br>
                  <h3>Generalization to dynamic settings</h3>
                  <p class="lead text-justify">
                        While the model is trained in static scenes, it demonstrates direct generalizability to dynamic settings, including dynamic obstacle avoidance and crowd motion synthesis.
                       </p>
                        <div style="display: flex; justify-content: space-between; align-items: flex-end;">

        <div style="text-align: center;">
            <h6>Moving Obstacle</h6>
            <video style="width: 90%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/moving.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

        <div style="text-align: center;">
            <h6>Two People</h6>
            <video style="width: 90%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/2man.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

        <div style="text-align: center;">
            <h6>More People</h6>
            <video style="width: 90%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/4man.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

        <div style="text-align: center;">
            <h6>Even More People</h6>
            <video style="width: 90%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/8man.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

    </div>

           <br> <br>
            <p class="lead text-justify">
                        Besides, our generative human motion model allows for generating diverse crowd motions for diverse body shapes, incorporating variations in height and weight.
                       </p>

                       <div style="display: flex; justify-content: space-between; align-items: flex-end;">

        <div style="width: 30%; text-align: center;">
            <h6>PhysicsVAE [Won et al. 2022]</h6>
            <video style="width: 100%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/physicsvae.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

        <div style="width: 30%; text-align: center;">
            <h6>Ours</h6>
            <video style="width: 100%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/diverse.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

        <div style="width: 30%; text-align: center;">
            <h6>Static body shapes in the left video</h6>
            <img class="img-fluid" src="images/shape.png">
        </div>

    </div>

               </div>
            </div>
         </div>

     <section id="rendering" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                   <h2>Overview of EgoGen</h2> <br>
                  <div class="text-center">
                     <!-- <p><img class="img-fluid" alt="teaser" src="https://via.placeholder.com/1500x600"></p> -->
<!--                     <p><img class="img-fluid" alt="teaser" src="images/teaser.png"></p>-->
                       <p class="lead text-justify">
<!--                           <b>Dataset Overview</b>. -->
                           Through generative motion synthesis (<font color="green">green</font>), we further enhance egocentric data diversity by randomly sampling diverse body textures (ethnicity, gender) and 3D textured clothing through an automated clothing simulation pipeline (<font color="red">red</font>). With high-quality scenes and different egocentric cameras including multi-camera rigs (<font color="orange">orange</font>), we can render photorealistic egocentric synthetic data with rich and accurate ground truth annotations.
                  </p>
                  </div>
                   <div class="text-center" style="width: 60%; margin: auto;">
                     <p><img class="img-fluid" alt="pipeline overview" src="images/pipeline.jpg"></p>
                  </div>
<!--                   <p class="lead text-justify">-->
<!--                       TODO-->
<!--                   </p>-->
               </div>
            </div>
         </div>
      </section>


      <section id="applications" class="">
         <div class="container" id="lamar">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                   <h2>Applications</h2> <br>
                   <h3>Mapping and Localization for HMDs</h3>
                        
<!--
                        <p class="lead text-justify">EgoGen has the capability to produce realistic synthetic egocentric data that closely matches real-world captures.</p>
                        <div style="display: flex; justify-content: center; gap:50px;">

        <div>
            <h6 style="text-align: center;">LaMAR [Sarlin et al. 2022]</h6>
            <img src="images/lamar.gif" alt="Real-world captures with HMDs" height="300">
        </div>

        <div>
            <h6 style="text-align: center;">EgoGen synthetic data</h6>
            <img src="images/egogen.gif" alt="EgoGen synthetic image data" height="300">
        </div>

    </div>-->
                    <p class="lead text-justify">EgoGen has the capability to produce realistic synthetic egocentric data that closely matches real-world captures. We visualize some synthetic images and their extracted feature points with SuperPoint:</p>

                    <div style="display: flex; justify-content: center; gap:10px; width: 80%; margin: auto;">

        <div>
            <img src="images/lamar/1.png" alt="Real-world captures with HMDs" class="img-fluid">
        </div>

        <div>
            <img src="images/lamar/2.png" alt="Real-world captures with HMDs" class="img-fluid">
        </div>

        <div>
            <img src="images/lamar/3.png" alt="Real-world captures with HMDs" class="img-fluid">
        </div>

    </div>

    <h5 class="img-wide text-center">
                        <font color="#6F6D6D " size="4"><font color="red">Red</font> denotes detected 2D feature points and <font color="magenta">magenta</font> denotes triangulated 3D points.</font>
                        <h5>

<br>
                    <p class="lead text-justify">And we show some render-to-real image pairs matched with SuperGlue:</p>

                    <div style="display: flex; justify-content: center; gap:10px; width: 80%; margin: auto;">

        <div>
            <img src="images/lamar/match1.png" alt="Real-world captures with HMDs" class="img-fluid">
        </div>

        <div>
            <img src="images/lamar/match2.png" alt="Real-world captures with HMDs" class="img-fluid">
        </div>

    </div>

<br>
                    <p class="lead text-justify" id="hmr">EgoGen can let virtual humans explore large-scale scenes, render dense egocentric views, and build a more complete SfM map, and it holds promise for creating AR mapping and localization datasets for digital twin scenes without manual data collection, providing enhanced privacy preservation.</p>
                   <br>
                   <h3>Human Mesh Recovery from Egocentric Views</h3>
                   <p class="lead text-justify">We simulate the data collection process of Egobody and let two virtual humans walk in the scanned scene meshes from Egobody. We randomly sample <em> gender</em>, <em> body shape </em>, and <em> initial body pose </em> and synthesize human motions with our proposed generative human motion model to increase data diversity. For RGB data generation, to further increase data diversity and close the sim-real gap, we randomly sample body texture and 3D textured clothing meshes from BEDLAM and perform automated clothing simulation. The model exhibits notable improvement with large-scale egocentric synthetic data.</p>
                   <h4>HMR from Depth</h4>
                   <h5>Synthetic data samples</h5>
                    <div style="text-align: center; width: 50%; margin: auto;"><img src="images/depth_sample.png" alt="Real-world captures with HMDs" class="img-fluid"></div>

                    <h5 class="img-wide text-center">
                        <font color="#6F6D6D " size="4">Row 1: Synthetic depth images for HMR. Row 2: Images with simulated sensor noise.</font>
                        <h5>

                   <h5>Qualitative results</h5> 

                    <div style="display: flex; justify-content: space-between;">

        <div>
            <video style="width: 100%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/hmr1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

        <div>
            <video style="width: 100%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/hmr2.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

    </div>

                   <br>
                   <h4>HMR from RGB</h4>
                   <h5>Synthetic data samples</h5>

                    <div style="text-align: center; width: 60%; margin: auto;"><img src="images/rgb_sample.png" alt="Real-world captures with HMDs" class="img-fluid"></div>

                    <h5 class="img-wide text-center">
                        <font color="#6F6D6D " size="4">Row 1: Synthetic RGB images for HMR. Row 2: Images with simulated motion blur.</font>
                        <h5>

                   <h5>Qualitative results</h5>

                    <div style="display: flex; justify-content: space-between;">

        <div>
            <video style="width: 100%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/hmr3.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>     
                   
        <div>      
            <video style="width: 100%; height: auto; display: block; margin: auto;" controls autoplay loop muted playsinline>
                <source src="video/hmr4.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

    </div>

                  </div>
               </div>
            </div>
         </div>
      </section>


      <section id="video" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">Video</h2>
                  <div class="embed-responsive embed-responsive-16by9">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/-WKQ4Vx2AXU?si=T5bcu9-3m-jisFMj" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                  </div>
               </div>
            </div>
         </div>
      </section>

				
      <!--<section id="downloads" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                   <h2>Downloads</h2> <br>
                  <h2 class="img-wide text-center">
                      <a class="publink" href="" target="_blank" style="text-decoration: none"> Arxiv <i class="fa fa-print"></i></a> &nbsp;&nbsp;
                      <a class="publink" href="" target="_blank" style="text-decoration: none"> Code <i class="fa fa-github"></i></a> &nbsp;&nbsp;
                      </h2>
               </div>
            </div>
         </div>
      </section>-->



      <section id="citation" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">Citation</h2>
                  <br>
                  <a class="publink" target="_blank" href="https://arxiv.org/abs/2401.08739"><b>
                    EgoGen: An Egocentric Synthetic Data Generator </b><br></a>
                    Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan Zhang, Marc Pollefeys, Siyu Tang<br>
                  <br><br>
<pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px">
@InProceedings{li2024egogen,
    author    = {Li, Gen and Zhao, Kaifeng and Zhang, Siwei and Lyu, Xiaozhong and Dusmanu, Mihai and Zhang, Yan and Pollefeys, Marc and Tang, Siyu},
    title     = {EgoGen: An Egocentric Synthetic Data Generator},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {14497-14509}
}
</pre>
               </div>
            </div>
         </div>
      </section>




      <!--<section id="team" class="team-section">
         <div class="container">
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <h2 class="section-title-tc">Team</h2>
                        <div class="text-center">
                         <table>
                               <tr>
                                   <th> <img src="images/teams/siwei.jpg" width="150" height="150" border="0">  </th>
                                   <th> <img src="images/teams/qianli.jpg" width="150" height="150" border="0">  </th>
                                   <th> <img src="images/teams/yan.jpg" width="150" height="150" border="0"> </th>
                                   <th> <img src="images/teams/sadegh.jpg" width="150" height="150" border="0">  </th>
                                   <th> <img src="images/teams/darren.jpg" width="150" height="150" border="0"> </th>
                                   <th> <img src="images/teams/siyu.jpg" width="150" height="150" border="0"> </th>
                               </tr>

                               <tr>
                                   <td> <a href="https://vlg.inf.ethz.ch/team/Siwei-Zhang.html">Siwei Zhang</a> </td>
                                   <td> <a href="https://qianlim.github.io/">Qianli Ma</a>  </td>
                                   <td> <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a> </td>
                                   <td> <a href="https://sadegh-aa.github.io/">Sadegh Aliakbarian</a> </td>
                                   <td> <a href="http://www.cs.bath.ac.uk/~dpc/">Darren Cosker</a> </td>
                                   <td> <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a> </td>
                               </tr>
                         </table>

                        </div>
                  </div>
               </div>
            </div>
      </section>-->

      <section id="contact" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
	                <h2>Contact</h2>
		            <br>For questions, please contact Gen Li:<br><a href="mailto:gen.li@inf.ethz.ch">gen.li@inf.ethz.ch</a>
               </div>
            </div>
         </div>
      </section>

      <!-- Footer -->
      <footer class="py-5 bg-dark">
         <div class="container">
            <p class="m-0 text-center text-white">Copyright &copy; VLG 2023</p>
             <p style="text-align:right;font-size:small;" class="text-white">
            template from <a href="https://neuralbodies.github.io/LEAP/index.html">LEAP</a>
         </div>
         <!-- /.container -->
      </footer>
      <!-- Bootstrap core JavaScript -->
      <script src="vendor/jquery/jquery.min.js"></script>
      <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
      <!-- Plugin JavaScript -->
      <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
      <!-- Custom JavaScript for this theme -->
      <script src="js/scrolling-nav.js"></script>
   </body>
</html>
